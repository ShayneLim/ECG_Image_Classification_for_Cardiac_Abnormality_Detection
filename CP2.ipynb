{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f04a7b-d67e-4181-8f60-3003c20232cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export train:   0%|                          | 50/17084 [00:03<17:24, 16.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] saved 50 images → /Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export val:   1%|▎                            | 20/2146 [00:01<02:02, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] saved 20 images → /Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export test:   1%|▎                           | 20/2158 [00:01<02:06, 16.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] saved 20 images → /Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/test\n",
      "/Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/train/MI/77.png\n",
      "/Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/train/MI/50.png\n",
      "/Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/train/STTC/22.png\n",
      "/Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/train/STTC/54.png\n",
      "/Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/train/HYP/45.png\n"
     ]
    }
   ],
   "source": [
    "import os, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wfdb\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "# set base folder\n",
    "DB = r\"/Users/shayne/Documents/SUNWAY_UNI/sem8/capstone 1/dataset/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3\" # <- path to the PTB-XL folder\n",
    "OUT_ROOT = r\"/Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images\"        # png saved here\n",
    "\n",
    "FNAME_COL = \"filename_lr\"                      \n",
    "SR = 100      \n",
    "# 1600 x 1200\n",
    "TARGET_SIZE = (800, 600)                       # output image width × height (pixels)\n",
    "\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "\n",
    "# read metadata CSVs\n",
    "df = pd.read_csv(os.path.join(DB, \"ptbxl_database.csv\"))\n",
    "scp = pd.read_csv(os.path.join(DB, \"scp_statements.csv\"), index_col=0)\n",
    "\n",
    "# keep only diagnostic statements\n",
    "diagnostic_codes = scp[scp[\"diagnostic\"] == 1]\n",
    "\n",
    "\n",
    "#map scp_codes convert to diagnostic superclasses\n",
    "def to_superclasses(scp_codes_str):\n",
    "    codes = ast.literal_eval(scp_codes_str)  # dict: code → weight\n",
    "    diags = [c for c in codes.keys() if c in diagnostic_codes.index]\n",
    "    supers = sorted({diagnostic_codes.loc[c, \"diagnostic_class\"] for c in diags})\n",
    "    return supers\n",
    "\n",
    "df[\"superclasses\"] = df[\"scp_codes\"].apply(to_superclasses)\n",
    "\n",
    "# official stratified folds\n",
    "\n",
    "train_df = df[df[\"strat_fold\"].isin(range(1, 9))].copy()\n",
    "val_df   = df[df[\"strat_fold\"] == 9].copy()\n",
    "test_df  = df[df[\"strat_fold\"] == 10].copy()\n",
    "\n",
    "\n",
    "#single-label primary class per record\n",
    "# if an entry has multiple super classes, pick one based on the priority shown below\n",
    "PRIORITY = [\"MI\", \"STTC\", \"HYP\", \"CD\", \"NORM\"]\n",
    "\n",
    "# function that picks a super class \n",
    "def choose_primary_superclass(superclasses):\n",
    "    if not superclasses:\n",
    "        return None\n",
    "    for c in PRIORITY:\n",
    "        if c in superclasses:\n",
    "            return c\n",
    "    return superclasses[0]\n",
    "\n",
    "# create a new column called primary class and apply the result of the super class function to it\n",
    "for split in (train_df, val_df, test_df):\n",
    "    split[\"primary_class\"] = split[\"superclasses\"].apply(choose_primary_superclass)\n",
    "\n",
    "# drop the rows that have no primary class \n",
    "train_df = train_df.dropna(subset=[\"primary_class\"])\n",
    "val_df   = val_df.dropna(subset=[\"primary_class\"])\n",
    "test_df  = test_df.dropna(subset=[\"primary_class\"])\n",
    "\n",
    "\n",
    "# helpers for reading WFDB and saving plots\n",
    "\n",
    "def load_signal_and_leads(rec_rel_path, base_dir=DB):\n",
    "    \"\"\"Read WFDB record. Returns (signal[T,12], lead_names[list]).\"\"\"\n",
    "    rec_path = os.path.join(base_dir, rec_rel_path)\n",
    "    sig, meta = wfdb.rdsamp(rec_path)\n",
    "    names = list(meta.sig_name) if hasattr(meta, \"sig_name\") else [f\"Lead{i+1}\" for i in range(sig.shape[1])]\n",
    "    return sig.astype(\"float32\"), names\n",
    "    \n",
    "\n",
    "# algorithm that turns the waveform data into plots\n",
    "# 12 leads per image in this case\n",
    "def save_12lead_strip(signal, lead_names, out_path, sr=SR, target_size=TARGET_SIZE):\n",
    "    \"\"\"Plot 12 leads in a 3×4 grid and save as PNG.\"\"\"\n",
    "    T, C = signal.shape\n",
    "    fig_w, fig_h = 10, 6\n",
    "    dpi = min(target_size[0]/fig_w, target_size[1]/fig_h)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(fig_w, fig_h), dpi=dpi)\n",
    "    axes = axes.ravel()\n",
    "    t = np.arange(T) / float(sr)\n",
    "\n",
    "    for i in range(min(C, 12)):\n",
    "        ax = axes[i]\n",
    "        ax.plot(t, signal[:, i], linewidth=0.8)\n",
    "        # ax.set_title(lead_names[i] if i < len(lead_names) else f\"Lead {i+1}\", fontsize=8)\n",
    "        ax.set_xlim([t[0], t[-1]])\n",
    "        ax.axis(\"off\")\n",
    "    for j in range(C, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(pad=0.15)\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\", pad_inches=0.03)\n",
    "    plt.close(fig)\n",
    "\n",
    "# saves the plots into iamges with good file directory and names\n",
    "def export_split_images(split_df, split_name, limit=None):\n",
    "    \"\"\"Save ECG plots into OUT_ROOT/split_name/<class>/<ecg_id>.png.\"\"\"\n",
    "    saved = 0\n",
    "    for _, row in tqdm(split_df.iterrows(), total=len(split_df), desc=f\"Export {split_name}\"):\n",
    "        if limit and saved >= limit:\n",
    "            break\n",
    "        label = row[\"primary_class\"]\n",
    "        if not label:\n",
    "            continue\n",
    "        try:\n",
    "            signal, leads = load_signal_and_leads(row[FNAME_COL])\n",
    "        except Exception as e:\n",
    "            # print(\"Failed:\", row[FNAME_COL], e)\n",
    "            continue\n",
    "\n",
    "        ecg_id = int(row[\"ecg_id\"]) if \"ecg_id\" in row else _\n",
    "        out_path = os.path.join(OUT_ROOT, split_name, label, f\"{ecg_id}.png\")\n",
    "        save_12lead_strip(signal, leads, out_path)\n",
    "        saved += 1\n",
    "    print(f\"[{split_name}] saved {saved} images → {os.path.join(OUT_ROOT, split_name)}\")\n",
    "\n",
    "\n",
    "# run export (try small limits first)\n",
    "export_split_images(train_df, \"train\", limit=50)\n",
    "export_split_images(val_df,   \"val\",   limit=20)\n",
    "export_split_images(test_df,  \"test\",  limit=20)\n",
    "\n",
    "\n",
    "# preview a few saved images\n",
    "some = glob.glob(os.path.join(OUT_ROOT, \"train\", \"*\", \"*.png\"))[:5]\n",
    "for p in some:\n",
    "    print(p)\n",
    "    img = Image.open(p)\n",
    "    img.show()  # opens in default image viewer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bbf0fa7-3f07-494d-afae-63843f5d7f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 50 rows → /Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/train_labels.csv\n",
      "Wrote 20 rows → /Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/val_labels.csv\n",
      "Wrote 20 rows → /Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images/test_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "CLASS_ORDER = [\"NORM\", \"MI\", \"STTC\", \"HYP\", \"CD\"]\n",
    "\"\"\"\n",
    "the class order above doesnt really matter (this just seems right to me)\n",
    "what the function below does is maps the class into binary as below\n",
    "input: ['CD', 'HYP']\n",
    "output: {\n",
    "  'NORM': 0,\n",
    "  'MI': 0,\n",
    "  'STTC': 0,\n",
    "  'HYP': 1,\n",
    "  'CD': 1\n",
    "}\n",
    "\n",
    "return {NORM: 1, MI: 0}\n",
    "\"\"\"\n",
    "# basically this produces a a format that the neural network understands \n",
    "# CLASS_ORDER = [\"NORM\", \"MI\", \"STTC\", \"HYP\", \"CD\"]\n",
    "# [0, 1, 1, 0, 0]\n",
    "def to_multihot(superclasses):\n",
    "    s = set(superclasses)\n",
    "    return {c: int(c in s) for c in CLASS_ORDER}\n",
    "\n",
    "def build_labels_csv_from_existing(split_df, split_name, out_root=OUT_ROOT):\n",
    "    \"\"\"\n",
    "    create scvs with multihots corresponding to each ecg image, this data should be ready\n",
    "    to be passed as \"tensors\" into the neural network (might have to double check if any steps remain)\n",
    "\n",
    "    u can think of it like this:\n",
    "    we currently have images stored in our local, we're basically:\n",
    "    1. locating the images that we have stored\n",
    "    2. referring back to the main dataset csv file to check the superclasses of these images\n",
    "    3. creating a csv that is neural network ready\n",
    "    4. this csv only includes 3 columns [imagepath, superclass, multihot]\n",
    "    5. we'll feed this into the neural network\n",
    "\n",
    "    multihot is basically just us mapping the superclasses into a dictionary\n",
    "\n",
    "    eg: if one of our entries's superclass look like ['CD', 'HYP']\n",
    "    the multihot just does {NORM: 0, MI: 0, STTC: 0, HYP: 1, CD: 1, }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # empty rows to store all entries\n",
    "    rows = []\n",
    "    # the directory of the split (train/val/test)\n",
    "    split_dir = os.path.join(out_root, split_name)\n",
    "    # C:\\Users\\User\\ecg_images\\train\\NORM\\1.png\n",
    "    # for each entry in the dataframe [id, name, superclass, primaryclass]\n",
    "    for _, r in split_df.iterrows():\n",
    "        # we get the super class\n",
    "        supers = r[\"superclasses\"]          # e.g., ['CD','HYP']\n",
    "        # supers = NORM\n",
    "        # if no superclass, then this entry is meaningless\n",
    "        if not supers:\n",
    "            continue\n",
    "        # get primary class\n",
    "        \n",
    "        primary = r[\"primary_class\"]\n",
    "        # get the id of the ecg\n",
    "        ecg_id = int(r[\"ecg_id\"])\n",
    "        # find the image that we saved that corresponds to the entry we're lookniga t right now\n",
    "        # train / NORM / 1.png\n",
    "        img_path = os.path.join(split_dir, primary, f\"{ecg_id}.png\")\n",
    "        if not os.path.exists(img_path):\n",
    "            # might not exist if you used a small 'limit' during export\n",
    "            continue\n",
    "        # C:\\Users\\User\\ecg_images\\train\\NORM\\1.png\n",
    "        # creates a multihot row\n",
    "        mh = to_multihot(supers)\n",
    "        row = {\n",
    "            \"image_path\": img_path.replace(\"\\\\\", \"/\"),\n",
    "            \"labels\": json.dumps(sorted(supers))\n",
    "        }   \n",
    "        # add the columns\n",
    "        row.update(mh)                      # add NORM/MI/STTC/HYP/CD columns\n",
    "        # add the rows\n",
    "        rows.append(row)\n",
    "        # we're essentially building a dataframe that has [imagepath, superclass, [superclasses_multihot]]\n",
    "\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    out_csv = os.path.join(out_root, f\"{split_name}_labels.csv\")\n",
    "    df_out.to_csv(out_csv, index=False)\n",
    "    print(f\"Wrote {len(df_out)} rows → {out_csv}\")\n",
    "    return out_csv\n",
    "\n",
    "# build csvs for all splits\n",
    "train_csv = build_labels_csv_from_existing(train_df, \"train\")\n",
    "val_csv   = build_labels_csv_from_existing(val_df,   \"val\")\n",
    "test_csv  = build_labels_csv_from_existing(test_df,  \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe4cddf0-a05f-43ec-9f58-78550cdbaf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "# === Paths (update OUT_ROOT to your folder) ===\n",
    "OUT_ROOT = r\"/Users/shayne/Documents/SUNWAY_UNI/sem8/Capstone2/ecg_images\" \n",
    "train_csv = os.path.join(OUT_ROOT, \"train_labels.csv\")\n",
    "val_csv   = os.path.join(OUT_ROOT, \"val_labels.csv\")\n",
    "test_csv  = os.path.join(OUT_ROOT, \"test_labels.csv\")\n",
    "\n",
    "# Class order used everywhere\n",
    "CLASS_ORDER = [\"NORM\", \"MI\", \"STTC\", \"HYP\", \"CD\"]\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ab74920-3080-42cf-8afa-e0464567e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelECGImages(Dataset):\n",
    "    \"\"\"\n",
    "    Reads rows: image_path, labels(JSON list), and returns:\n",
    "      image tensor, multi-hot target tensor of shape [5]\n",
    "    \"\"\"\n",
    "    def __init__(self, labels_csv, transform=None):\n",
    "        self.df = pd.read_csv(labels_csv)\n",
    "        # Read csv and turn paths and labels into lists\n",
    "        # [a,b,c]\n",
    "        # [x,y,z]\n",
    "        self.paths = self.df[\"image_path\"].tolist()\n",
    "        self.labels_json = self.df[\"labels\"].tolist()\n",
    "        # transform function to apply on the data \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self): \n",
    "        # returns how many images/data we have\n",
    "        return len(self.paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        # Parse labels JSON → list[str] → multi-hot\n",
    "        # load the labels\n",
    "        # [NORM, MI] or other variations like [MI, STTC]\n",
    "        # json loads turns a string into an list \n",
    "        labs = json.loads(self.labels_json[idx])\n",
    "        # create a \"hot\" arrray of size CLASS_ORDER filled with zeros\n",
    "        # [NORM, \n",
    "        # [0,0,0,0,0]\n",
    "        target = torch.zeros(len(CLASS_ORDER), dtype=torch.float32)\n",
    "        # target = [0.0, 1.0, 0.0, 0.0, 0.0]\n",
    "        # for every label that matches with class order \n",
    "        # [\"NORM\", \"MI\", \"STTC\", \"HYP\", \"CD\"], fill with 1.0\n",
    "        # [NORM, MI] -> [1.0, 1.0, 0.0, 0.0, 0.0]\n",
    "        # labs = [NORM, MI]\n",
    "        for lab in labs:\n",
    "            if lab in CLASS_ORDER:\n",
    "                target[CLASS_ORDER.index(lab)] = 1.0\n",
    "        # [Image, label]\n",
    "        # [Image, [0.0, 1.0, 0.0, 0.0, 0.0]]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45ab094b-1cfd-45b6-a781-4041b7cf12bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 20, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "train_tfms = transforms.Compose([ #Chains multiple inmage transformation tgt in a single pipeline\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    # Light augmentation (avoid horizontal flip; it reverses time)\n",
    "    # We want it to understand small shifts in width and height, or small changes in zoom, but we do not want to make it rotate\n",
    "    # rotation matters in ecg\n",
    "    transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.02,0.02), scale=(0.98,1.02))], p=0.5),\n",
    "    # turns it into a tensor floatTensor to be exact\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# repeat the same for testing images and evaluation images\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_ds = MultiLabelECGImages(train_csv, transform=train_tfms)\n",
    "val_ds   = MultiLabelECGImages(val_csv,   transform=eval_tfms)\n",
    "test_ds  = MultiLabelECGImages(test_csv,  transform=eval_tfms)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0  \n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c4e175a-e4f0-4fcb-b7f0-5e4be0ccb3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1628, 24.0000, 24.0000, 49.0000,  9.0000], device='mps:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "the classes are not balanced, there are way more NORM than everything else\n",
    "assume a 75-25 ratio. due to this imbalance, we must make the model care \n",
    "just as much about the minority (25) through heavy penalties\n",
    "\"\"\"\n",
    "def compute_pos_weight(labels_csv):\n",
    "    df = pd.read_csv(labels_csv)\n",
    "    # Expect columns NORM, MI, STTC, HYP, CD as 0/1 flags\n",
    "    # load training labels only because thats what we train on\n",
    "    # count the number of positives per class\n",
    "    # returns smth like [1200,80,250,200,90] just an example\n",
    "    counts_pos = df[CLASS_ORDER].sum().values.astype(np.float32) \n",
    "    # find out which classs are rare or common\n",
    "    counts_all = len(df)\n",
    "    counts_neg = counts_all - counts_pos\n",
    "    # Avoid division by zero\n",
    "    # calculate the weights\n",
    "    pos_weight = counts_neg / np.clip(counts_pos, 1.0, None)\n",
    "    # the weights tell us how heavily we should penalize for failing to recognize \n",
    "    # certain classes (what does this remind u of?)\n",
    "    return torch.tensor(pos_weight, dtype=torch.float32)\n",
    "\n",
    "pos_weight = compute_pos_weight(train_csv).to(device)\n",
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5094981-1e40-4325-8cfe-235e4876cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Two 3×3 convs with BN+ReLU. Keeps H,W; downsampling happens in the following MaxPool.\"\"\"\n",
    "\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        # input channel = 3 (RGB), filter number (3x3 stencil), padding to make sure stencil stays on edge\n",
    "        # bias = False, we'll handle it in bn1\n",
    "        # convolution runs a filter on an input noting its variations, patterns, nuances, etc\n",
    "        # we do this many times across many layers to \"learn\" a pattern\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1, bias=False)\n",
    "        # batch normalizer stabilizes and normalizes activations\n",
    "        # recentering / rescaling inputs to a predictable range\n",
    "        # activations (outputs) -> channel/node (layer) \n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        # repeats but this time instead of 3 -> 32, we'll look 32 -> 32\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # previously we defined how our model should work\n",
    "        # forward function runs our model exactly n times (given you call it n times)\n",
    "        # relu (adds non linearity)\n",
    "        # image -> conv1 -> bn -> relu -> features\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # conv → BN → ReLU\n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # conv → BN → ReLU\n",
    "        return x\n",
    "\n",
    "class ECGStripCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple-but-solid CNN for 224x224 RGB images:\n",
    "    4 stages (channels: 32, 64, 128, 256) with MaxPool after each stage,\n",
    "    then global average pooling and a small classifier head to 5 logits.\n",
    "\n",
    "    basically: \n",
    "    pattern spotters - convolutions\n",
    "    zoom-out step - pooling \n",
    "    summary clipboard - global average pooling\n",
    "    final judge - linear head that outputs 5 scores (1 per diagnosis)\n",
    "\n",
    "    input: [B, 3, 224, 224] (B (optional), C, H, W)\n",
    "    output: [B, 5] logits (5 classes)\n",
    "\n",
    "    conv block basically marks every possible pattern/evidence it can find\n",
    "    maxpool grabs the most convincing/relevant one from each segment (igores noise)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=5, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        # conv block outputs 32 pattern maps\n",
    "        self.stage1 = ConvBlock(3,32)   # [B,3,224,224] -> [B,32,224,224]\n",
    "        # for all the pattterns maxpool (2x2) allows the maxpool\n",
    "        # filter to traverse and generalize\n",
    "        # after each step, the resolution gets smaller and smaller\n",
    "        # but this is fine because even with fewer piels, the features are more generalized\n",
    "        self.pool1  = nn.MaxPool2d(2)      #                  -> [B,32,112,112]\n",
    "\n",
    "        # 32 - 64 - 128 - 256 (are the features)\n",
    "        # why more channels as we move on? \n",
    "        self.stage2 = ConvBlock(32,  64)   # -> [B,64,112,112]\n",
    "        self.pool2  = nn.MaxPool2d(2)      # -> [B,64,56,56]\n",
    "\n",
    "        self.stage3 = ConvBlock(64, 128)   # -> [B,128,56,56]\n",
    "        self.pool3  = nn.MaxPool2d(2)      # -> [B,128,28,28]\n",
    "\n",
    "        self.stage4 = ConvBlock(128,256)   # -> [B,256,28,28]\n",
    "        self.pool4  = nn.MaxPool2d(2)      # -> [B,256,14,14]\n",
    "\n",
    "        # Global average pooling → [B,256,1,1]\n",
    "        # we already have many pools, the function above just grabs the average\n",
    "        # adaptive because regardless of inputsize it always returns a 1x1\n",
    "        self.gap    = nn.AdaptiveAvgPool2d((1,1))\n",
    "        # randomly zeros a fraction p\n",
    "        # prevents classifier on depending on a small set of features \n",
    "        self.drop   = nn.Dropout(p=dropout_p)\n",
    "        # linear layer to finish it off \n",
    "        # conv blocks: 32 -> 64 -> 128 -> 256\n",
    "        # linear block: 256 -> 5 (5 classes)\n",
    "        self.fc     = nn.Linear(256, n_classes)  # 5 logits for multi-label\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.stage1(x))\n",
    "        x = self.pool2(self.stage2(x))\n",
    "        x = self.pool3(self.stage3(x))\n",
    "        x = self.pool4(self.stage4(x))\n",
    "        # calculates the average pooling\n",
    "        x = self.gap(x)                 # shape: [B,256,1,1] output: \n",
    "        # flatten is just reshaping the output into torch understandable ones\n",
    "        x = torch.flatten(x, 1)         # [B,256]\n",
    "        # zeros features\n",
    "        x = self.drop(x)\n",
    "        # get the actual logits\n",
    "        logits = self.fc(x)             # [B,5] (raw scores, not sigmoid) [0.42,0.456,0.5387,..,..\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "051fb989-001f-4e8a-98a4-785403a8870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer=None):\n",
    "    \"\"\"\n",
    "    Train if optimizer is provided; else evaluate.\n",
    "    Returns: average loss, list of all targets, list of all probs\n",
    "    \"\"\"\n",
    "    train_mode = optimizer is not None\n",
    "    model.train() if train_mode else model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_targets, all_probs = [], []\n",
    "\n",
    "    with torch.set_grad_enabled(train_mode):\n",
    "        for imgs, targets in loader: # train_df\n",
    "          \n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            # get our initial predictions\n",
    "            logits = model(imgs)               # [B,5]\n",
    "            # calculate loss\n",
    "            loss = loss_function(logits, targets)  # scalar\n",
    "\n",
    "            # our usual step \n",
    "            if train_mode:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "            # store probs for metrics 2.581 -> 0.43\n",
    "            # turn logits into probabilities called at [0,1]\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy() # detach moves probs from gpu to cpu to turn it into numpy array [1,2,3,4]\n",
    "            # we dont want to use gpu unless its for parallel tasks\n",
    "\n",
    "            # caches the data\n",
    "            all_probs.append(probs)\n",
    "            all_targets.append(targets.detach().cpu().numpy())\n",
    "\n",
    "    N = len(loader.dataset)\n",
    "    avg_loss = total_loss / max(1, N)\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    return avg_loss, all_targets, all_probs\n",
    "\n",
    "def multilabel_metrics(y_true, y_prob, threshold=0.5):\n",
    "    \"\"\"\n",
    "    precision: of all the cases we called (flagged) positive, how many are truly positive?\n",
    "    recall: of all the positive cases that exist, how many did we catch?\n",
    "    \n",
    "    f1 (micro and macro): harmonic mean of prec & rec (fairness/balance score)\n",
    "\n",
    "    f1 macro: used to deal with imbalance. coast on common cases and fail on rarities (75-25)\n",
    "    f1 micro: similar to what we're used to. tally every answer/flags and judge\n",
    "\n",
    "    evaluate behavior not only results\n",
    "    \n",
    "    if macro is low but micro is high: fix rare cases\n",
    "    if both are low: more data (more batches, higher res, etc), ecg augmentations, better lr schedules\n",
    "    \"\"\"\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    # Per-class precision/recall/F1\n",
    "    # prec, rec, f1, support (no. occurences of each label in y_true)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "    macro_f1 = f1.mean()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    # AUROC per class (guard against classes with single label value)\n",
    "    \"\"\"\n",
    "\n",
    "    PR: precision vs recall (how clean are my positives as i try to catch more?)\n",
    "    use when positives are rare (we obviously want to let more data flood)\n",
    "    pr shows us the tradeoffs (more false alarms) when we raise our recall (accept more data)\n",
    "    punishes \"crying wolf\"\n",
    "\n",
    "    roc is like a thesis statement (do spam emails tend to get higher spam scores than normal emails)\n",
    "    pr is a proposed solution (if i flag more emails as spam to catch all spam (high recall), do i accidentally flag\n",
    "    a lot more real emails (low precision)?\n",
    "\n",
    "    F1 at a tuned threshold: “given a chosen operating point, how balanced are my actual yes/no decisions?\"    \n",
    "\n",
    "    these 3 don't necessarily work together but looking at all of them gives us multiple perspectives\n",
    "    \"\"\"\n",
    "    aurocs = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        if len(np.unique(y_true[:, i])) == 2:\n",
    "            aurocs.append(roc_auc_score(y_true[:, i], y_prob[:, i]))\n",
    "        else:\n",
    "            aurocs.append(float('nan'))\n",
    "    return {\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"per_class_f1\": dict(zip(CLASS_ORDER, f1)),\n",
    "        \"per_class_precision\": dict(zip(CLASS_ORDER, prec)),\n",
    "        \"per_class_recall\": dict(zip(CLASS_ORDER, rec)),\n",
    "        \"per_class_auroc\": dict(zip(CLASS_ORDER, aurocs)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cc8b7c4-f22e-4670-b67a-095f8e5a3e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Now we can safely use the float() method on the PyTorch tensor\u001b[39;00m\n\u001b[1;32m     34\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (y_prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Convert to binary predictions\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m metrics \u001b[38;5;241m=\u001b[39m multilabel_metrics(y_true, y_pred, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmacroF1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  microF1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val:\n",
      "Cell \u001b[0;32mIn[18], line 61\u001b[0m, in \u001b[0;36mmultilabel_metrics\u001b[0;34m(y_true, y_prob, threshold)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmultilabel_metrics\u001b[39m(y_true, y_prob, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    precision: of all the cases we called (flagged) positive, how many are truly positive?\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    recall: of all the positive cases that exist, how many did we catch?\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m    if both are low: more data (more batches, higher res, etc), ecg augmentations, better lr schedules\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m (y_prob \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# Per-class precision/recall/F1\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# prec, rec, f1, support (no. occurences of each label in y_true)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     prec, rec, f1, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "# Instantiate your custom model\n",
    "model = ECGStripCNN(n_classes=5, dropout_p=0.3).to(device)\n",
    "\n",
    "# Loss: multi-label\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # pos_weight from earlier step\n",
    "# Optimizer & scheduler (tweak LR as you like)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "# start with somewhat high LR, this helps learn faster and detect bigger patterns\n",
    "# once we start to reach a plateau on accuracy\n",
    "# the lr is adaptively reduced, smller learning rates help with finer pattern detection (in this case)\n",
    "# runs until the end of epochs if no plateau, but if plateau even after the below function, we quit process early to save time and resources\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "# Train as before\n",
    "EPOCHS = 10\n",
    "best_val = float('inf')\n",
    "best_path = os.path.join(OUT_ROOT, \"best_custom_cnn_multilabel.pt\")\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # training (as usual)\n",
    "    train_loss, _, _ = run_epoch(model, train_dl, optimizer=optimizer)   # uses your earlier utility\n",
    "    # validation is training on unseen data\n",
    "    val_loss, y_true, y_prob = run_epoch(model, val_dl, optimizer=None)\n",
    "    # our scheculer keeps a history of the losses\n",
    "    # if the losses dont improve, we adjust LR\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Convert probabilities to binary predictions using the threshold\n",
    "    # Fix: Convert NumPy array to PyTorch tensor before using float() method\n",
    "    if isinstance(y_prob, np.ndarray):\n",
    "        y_prob = torch.tensor(y_prob)\n",
    "    \n",
    "    # Now we can safely use the float() method on the PyTorch tensor\n",
    "    y_pred = (y_prob >= 0.5).float()  # Convert to binary predictions\n",
    "    \n",
    "    metrics = multilabel_metrics(y_true, y_pred, threshold=0.5)\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  \"\n",
    "          f\"macroF1={metrics['macro_f1']:.3f}  microF1={metrics['micro_f1']:.3f}\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save({\"model_state\": model.state_dict(),\n",
    "                    \"class_order\": CLASS_ORDER}, best_path)\n",
    "        print(f\"  ↳ saved best to {best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1643e75-b8bd-453d-8933-a11e786d1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\"\n",
    "FINE tuning:\n",
    "0. epochs\n",
    "1. dropout\n",
    "2. lr\n",
    "3. weight decay\n",
    "4. factor (scheduler)\n",
    "6. patience (scheduler)\n",
    "7. more feature classes in ConvBlock\n",
    "8. more layers for convblock\n",
    "9. image augmentation\n",
    "10. increase data size, increase image resolution, change to higher bitrate\n",
    "\"\"\"\n",
    "\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "test_loss, y_true_test, y_prob_test = run_epoch(model, test_dl, optimizer=None)\n",
    "test_metrics = multilabel_metrics(y_true_test, y_prob_test, threshold=0.5)\n",
    "\n",
    "# Compute simple accuracies\n",
    "threshold = 0.5\n",
    "y_pred_test = (y_prob_test >= threshold).astype(int)\n",
    "exact_match = np.all(y_pred_test == y_true_test, axis=1)\n",
    "exact_acc = exact_match.mean() * 100\n",
    "labelwise_acc = (y_pred_test == y_true_test).mean() * 100\n",
    "\n",
    "print(f\"\\nTest loss={test_loss:.4f}\")\n",
    "print(f\"MacroF1={test_metrics['macro_f1']:.3f}  MicroF1={test_metrics['micro_f1']:.3f}\")\n",
    "print(f\"Exact-match accuracy: {exact_acc:.2f}%\")\n",
    "print(f\"Label-wise mean accuracy: {labelwise_acc:.2f}%\")\n",
    "print(\"Per-class F1:\", test_metrics[\"per_class_f1\"])\n",
    "print(\"Per-class AUROC:\", test_metrics[\"per_class_auroc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeeb4fa-9de5-4da4-a438-1620b5070ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
